# fullNameOverride: k8s
datasource:
  auto_load_label: grafana-datasource
  isDefault: true
prometheus-operator:

  commonLabels:
    k8s-monitoring: "true"
  defaultRules:
    labels: &scrapeLabels
      k8s-monitoring: "true"
    create: true
    rules:
      alertmanager: true
      etcd: false
      general: true
      k8s: true
      kubeApiserver: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubeScheduler: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      node: true
      prometheusOperator: true
      prometheus: true

  prometheusOperator:
    createCustomResource: false
    configReloaderCpu: 5m
    configReloaderMemory: 25Mi

  prometheus:
    enabled: true
    ingress:
      enabled: true
      annotations:
        # Setting DNS name
        kubernetes.io/ingress.class: nginx-external
      hosts:
        - prometheus
    prometheusSpec:
      # serviceMonitorSelectorNilUsesHelmValues: false
      thanos:
        baseImage: improbable/thanos
        version: v0.2.1
        peers: k8s-thanos.system-monitoring.svc.cluster.local:10900
        objectStorageConfig:
          key: secret-file
          name: secret-name
      serviceMonitorSelector:
        matchLabels:
          *scrapeLabels
      ruleSelector:
        matchLabels:
          *scrapeLabels
      ruleNamespaceSelector:
        any: true
      serviceMonitorNamespaceSelector:
        any: true
      externalUrl: ""
      storageSpec: {}
      additionalAlertManagerConfigs:
      - kubernetes_sd_configs:
        - api_server: null
          role: service
        scheme: http
        path_prefix: /
        timeout: 10s
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_label_k8s_alertmanager]
          separator: ;
          regex: True
          replacement: $1
          action: keep
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          separator: ;
          regex: http
          replacement: $1
          action: keep
      additionalScrapeConfigs:
      - job_name: tiller
        scrape_interval: 1m
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: http
        kubernetes_sd_configs:
        - api_server: null
          role: pod
          namespaces:
            names: []
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_name]
          separator: ;
          regex: "tiller-deploy.*"
          replacement: $1
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          separator: ;
          regex: "http"
          replacement: $1
          action: keep
        - separator: ;
          regex: __meta_kubernetes_pod_label_(.+)
          replacement: $1
          action: labelmap
        - source_labels: [__meta_kubernetes_namespace]
          separator: ;
          regex: (.*)
          target_label: kubernetes_namespace
          replacement: $1
          action: replace
        - source_labels: [__meta_kubernetes_pod_name]
          separator: ;
          regex: (.*)
          target_label: kubernetes_pod_name
          replacement: $1
          action: replace

  alertmanager:
    ingress:
      enabled: true
      annotations:
        # Setting DNS name
        kubernetes.io/ingress.class: nginx-external
        # Putting prometheus behind oauth2 proxy (github login)
      hosts:
        - alertmanager
    alertmanagerSpec:
      externalUrl: ""

  grafana:
    enabled: true
    adminPassword: admin

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## Enable scraping the kubelet over https. For requirements to enable this see
      ## https://github.com/coreos/prometheus-operator/issues/926
      ##
      https: false

      ## Metric relabellings to apply to samples before ingestion
      ##
      cAdvisorMetricRelabelings: []
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      # 	relabel configs to apply to samples before ingestion.
      ##
      cAdvisorRelabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

    ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeControllerManager.endpoints only the port and targetPort are used
    ##
    service:
      port: 10252
      targetPort: 10252
      selector:
        component: kube-controller-manager

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: true
    service:
      port: 9153
      targetPort: 9153
      selector:
        k8s-app: kube-dns
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## 	metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

    ## If your etcd is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
    ##
    service:
      port: 2379
      targetPort: 2379
      selector:
        component: etcd

    ## Configure secure access to the etcd cluster by loading a secret into prometheus and
    ## specifying security configuration below. For example, with a secret named etcd-client-cert
    ##
    ## serviceMonitor:
    ##   scheme: https
    ##   insecureSkipVerify: false
    ##   serverName: localhost
    ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      scheme: http
      insecureSkipVerify: true
      serverName: ""
      caFile: ""
      certFile: ""
      keyFile: ""

      ## 	metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: true

    ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeScheduler.endpoints only the port and targetPort are used
    ##
    service:
      port: 10251
      targetPort: 10251
      selector:
        component: kube-scheduler

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## 	metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace


  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    rbac:
      create: true
    podSecurityPolicy:
      enabled: true

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true

    ## Use the value configured in prometheus-node-exporter.podLabels
    ##
    jobLabel: jobLabel

    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## 	metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
      #   replacement: $1
      #   action: drop

      ## 	relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$


cluster_name: k8s

global_recievers:
  slack:
    name: ""
    url: ""
    channel: ""
  pagerduty:
    name: k8s_pg_alerts
    integration_key: ""

monitoring:
  enabled: true
  alerting:
    enabled: false

k8s-alert-router:
  alerts:
    selectors:
      secretName: ""
      alertManagerNamespace: ""
    receivers:
      warningReceiverName: ""
      # Leave empty to disable pagerduty/critical alerts
      criticalReceiverName: ""

thanos_ingress:
  enabled: false
  annotations: {}
  hostname: ""
  tls:
    secretName: ""
    hostname: ""

prometheus_cluster:
  enabled: false
  workers: 2
